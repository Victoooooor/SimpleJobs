{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victoooooor/SimpleJobs/blob/main/movenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BxYjV77TuRQl"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-K57-6CucuR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import files\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WrywQe7_ul-c"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def to_gif(images, fps):\n",
        "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
        "  imageio.mimsave('./animation.gif', images, fps=fps)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "      <progress\n",
        "          value='{value}'\n",
        "          max='{max}',\n",
        "          style='width: 100%'\n",
        "      >\n",
        "          {value}\n",
        "      </progress>\n",
        "  \"\"\".format(value=value, max=max))\n",
        "\n",
        "def show_video(video_path, video_width = 600):\n",
        "   \n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "\n",
        "# Load the input image.\n",
        "def get_pose(image, thresh = 0.2):\n",
        "  detection_threshold = thresh\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "  image_origin = copy.copy(image)\n",
        "  image = tf.cast(tf.image.resize_with_pad(\n",
        "      image, 256, 256), dtype=tf.int32)\n",
        "  _, image_height, image_width, channel = image_origin.shape\n",
        "  # print(image_height, image_width)\n",
        "\n",
        "  if channel != 3:\n",
        "    sys.exit('Image isn\\'t in RGB format.')\n",
        "  output = movenet(image)\n",
        "  people = output['output_0'].numpy()[:, :, :51].reshape((6, 17, 3))\n",
        "\n",
        "\n",
        "  if image_width > image_height:\n",
        "    # print('scaling')\n",
        "    dif = people - 0.5\n",
        "    people[:,:,0] = 0.5 + image_width/image_height * dif[:,:,0]\n",
        "  elif image_width < image_height:\n",
        "    # print('scaling')\n",
        "    dif = people - 0.5\n",
        "    people[:,:,1] = 0.5 + image_height/image_width * dif[:,:,1]\n",
        "\n",
        "\n",
        "  # Save landmarks if all landmarks were detected\n",
        "  ppl = []\n",
        "  for i in range(6):\n",
        "    # print(output['output_0'][0, i, -1])\n",
        "    if output['output_0'][0, i, -1] > detection_threshold:\n",
        "      ppl.append(people[i])\n",
        "\n",
        "  should_keep_image = len(ppl) > 0\n",
        "  if not should_keep_image:\n",
        "    print('No pose was confidentlly detected.')\n",
        "  #draw all\n",
        "  merged_img = np.squeeze(image_origin.numpy(), axis=0)\n",
        "  \n",
        "  for pp in ppl:\n",
        "    merged_img = draw_prediction_on_image(\n",
        "        merged_img, np.array([[pp]]), output_image_height=image_height)\n",
        "  return merged_img, ppl\n",
        "\n",
        "def get_vid(filename, fhandle, desti = 'processed.mp4', interval = 5):\n",
        "  video_file = desti\n",
        "  video = cv2.VideoCapture(filename)\n",
        "  if not video.isOpened():\n",
        "    sys.exit('video does not exist')\n",
        "  fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "  frame_num = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  video_writer = cv2.VideoWriter(video_file,fourcc,fps,(frame_width,frame_height))\n",
        "  print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "  \n",
        "  frame_counter = 0\n",
        "  while True:\n",
        "      ret, frame = video.read()\n",
        "      if ret == True:\n",
        "          tfframe= tf.convert_to_tensor(frame)\n",
        "          new_frame, data = get_pose(tfframe)\n",
        "          video_writer.write(new_frame)\n",
        "\n",
        "          if frame_counter % interval == 0:\n",
        "            data=np.delete(data,2,2)\n",
        "            data[:,:,[0,1]] = data[:,:,[1,0]]\n",
        "            np.savetxt(fhandle, data.flatten(),\n",
        "                       fmt='%.18e', newline=',')  \n",
        "            fhandle.write(b\"\\n\")\n",
        "          frame_counter += 1\n",
        "      if ret == False:\n",
        "          break\n",
        "  video.release()\n",
        "  video_writer.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  return video_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48wOM4ho2i6F",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
        "movenet = model.signatures['serving_default']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG5Rhdkv7kwk"
      },
      "outputs": [],
      "source": [
        "#params\n",
        "interval = 5 #meaning save to csv every 5 frames\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuPp1ahN-E9e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "text_name = 'pose.csv'\n",
        "try:\n",
        "  os.remove(text_name)\n",
        "except:\n",
        "  None\n",
        "with open(text_name, \"ab\") as csv:\n",
        "    # numpy.savetxt(csv, a)\n",
        "  gen = get_vid(filename, csv, interval = interval)\n",
        "  csv.close()\n",
        "audiofile = '_sound.mp3'\n",
        "withsound = 'output.mp4'\n",
        "!ffmpeg -i {filename} -f mp3 -ab 192000 -vn {audiofile}\n",
        "!ffmpeg -i {gen} -i {audiofile} -map 0:0 -map 1:0 -c:v copy -c:a copy {withsound}\n",
        "!zip -r file.zip {text_name} {withsound}\n",
        "files.download('file.zip')\n",
        "\n",
        "try:\n",
        "  os.remove('file.zip')\n",
        "  os.remove(text_name)\n",
        "  os.remove(filename)\n",
        "  os.remove(audiofile)\n",
        "  os.remove(gen)\n",
        "  os.remove(withsound)\n",
        "except:\n",
        "  None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNVTDwTnUitL0aZ4K+D5VK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}